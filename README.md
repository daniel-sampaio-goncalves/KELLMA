# ğŸ§¿ KELLMA Search
![Python](https://img.shields.io/badge/Python-3.14-3776AB)
![Streamlit](https://img.shields.io/badge/Streamlit-1.52-FF4B4B)
![Ollama](https://img.shields.io/badge/Ollama-1.5.1-000000)
![License](https://img.shields.io/badge/License-Apache_2.0-4CAF50)

*Keyword Embeddings Large Language Model Approved Search*

---

KELLMA Search is a fullâ€‘stack Python application designed to streamline exploration of **PubMed Central (PMC)** biomedical literature. It combines a Streamlit interface, with a modular, multiâ€‘stage backend pipeline, local LLM inference through Ollama, and parallelized execution. The system generates persistent DOCX/JSON reports with direct quotes extracted from the most relevant articles.

## Table of Contents
- [Why KELLMA?](#-why-kellma)
- [Key Features](#-key-features)
- [How KELLMA differs from typical RAG tools](#-how-kellma-differs-from-typical-rag-tools)
- [User Interface](#-user-interface)
- [High-Level Architecture](#-high-level-architecture)
- [Dependencies](#-dependencies)
- [Installation](#-installation)
- [System Requirements](#-system-requirements)
- [FAQ](#-faq)
- [Contributing](#-contributing)
- [License](#-license)


## ğŸ¯ Why KELLMA?
  Biomedical literature is growing at an overwhelming pace. Traditional keyword search misses nuance, and most cloudâ€‘based LLM systems are optimized for speed and cost. As a result, they tend to summarize broad concepts rather than produce grounded, referenceâ€‘level outputs. Running deep semantic filtering across **millions** of fullâ€‘text articles is computationally intensive and simply not feasible for many users in a hosted LLM environment. KELLMA bridges that gap by combining parallelized parsing, local embeddings, and multiâ€‘LLM validation to surface only the most relevant PMC content â€” fast, reproducibly, and fully offline. The result is a modular, highâ€‘precision system for biomedical literature scanning, semantic filtering, and automated summarization when contextual accuracy and explicit referencing truly matter.

## âœ¨ Key Features

- **Highâ€‘precision retrieval** across millions of PMC fullâ€‘text articles
- **Semantic matching** using Ollama embeddings for semantic scoring
- **Multiâ€‘LLM validation** to approve or reject retrieved chunks based on contextual accuracy
- **LLMâ€‘based summarization** of the final, approved references
- **Concurrent execution** through Pythonâ€™s `concurrent.futures` for scalable parallelism and high throughput
- **Modular, multiâ€‘stage pipeline** with clear boundaries for retrieval, RAG, validation, and summarization
- **Structured JSON/DOCX outputs** for clean, readyâ€‘toâ€‘use reports
- **Persistent logs** for reproducibility, traceability, and debugging
- **Cache layer** for fast restarts.
- **Userâ€‘friendly Streamlit interface** for launching searches, monitoring progress, and downloading results


## ğŸ†š How KELLMA differs from typical RAG tools
| Feature | Standard RAG/LLM | KELLMA |
|--------|---------------|--------|
| Designed for millions of PMC articles | âŒ **No** | âœ… **Yes** |
| Multiâ€‘LLM semantic approval | âŒ **No** | âœ… **Yes**|
| Exact citation retrieval | âŒ **No** | âœ… **Yes**|
| Grounded summaries of retrieved articles | âŒ **No** | âœ… **Yes**|
| Works fully offline |  âš ï¸ **Partial** | âœ… **Yes**|
| Persistent logs, cache, and Word/JSON report generation | âš ï¸ **Partial** | âœ… **Yes**|

## ğŸŒ User Interface
<details><summary></span></summary>

![KELLMA_UI](examples/KELLMA_UI.png)
</details>

---
## <span id="high-level-architecture">ğŸ“ High-Level Architecture</span>

KELLMA Search is designed to handle large volumes of biomedical literature efficiently. The system uses a Streamlit front end that launches a fully independent Python process in the background. This backend is modular, scalable, and organized into clear stage boundaries to support maintainability and future expansion.

The Search itself is performed by following a 4 step protocol: 
1. PMC article parsing, text chunking, and keyword retrieval
2. Embeddingâ€‘based semantic matching to identify relevant text chunks
3. Literature review by four different LLM models to validate the semantic accuracy of retrieved text
4. Summarization and final report generation using only the approved chunks and references

<br>

### ğŸ—ºï¸ Flowchart Architecture Overview
<details>
 <summary></summary>

```mermaid
flowchart TB

Browser["Browser â€” Streamlit UI"] --> app_py["app.py"]

app_py -->|writes/watches| Logs["./logs/*.log / *.txt"]
app_py -->|reads| Results["./results/*.json / *.docx"]
app_py -->|run pipeline| Terminal["Terminal (independent process)"]

Terminal --> Pipelineinit["pipeline_init.py"]
Pipelineinit <-->|logs| Logs
Pipelineinit <-->|cache| Cache["./cache/*.pkl"]
Pipelineinit --> PMCClass["PmcRetrievalPipeline"]

%% Sequential pipeline
PMCClass --> Stage1["Stage 1 â€” Fetch papers"]
Stage1 --> Stage2["Stage 2 â€” RAG"]
Stage2 --> Stage3["Stage 3 â€” LLM Approvals"]
Stage3 --> Stage4["Stage 4 â€” LLM Summary"]

%% Stage 1 internals
Stage1 --> procMulti["process_multiple_articles()"]
procMulti --> Interpreter["InterpreterPoolExecutor"]
Interpreter --> Parser["pmc_article_parser_functions.py"]
Parser --> PMCarticles["PMC Articles"]

%% Stage 2 internals
Stage2 --> runBatches["run_in_batches()"]
runBatches --> ThreadPool["ThreadPoolExecutor"]
ThreadPool --> prepareList["prepare_list_to_run()"]
prepareList --> RagClass["Refined_Semantic_matching"]
RagClass --> |embedding scores|run_embedding_scores["run_embedding_scores()"]
run_embedding_scores --> Ollama["Ollama Local LLM"]

%% Stage 3 internals
Stage3 --> runLLM["run_LLM()"]
runLLM --> RagClass
RagClass --> |LLM approvals|Check_LLM_Semantics_sequentially["Check_LLM_Semantics_sequentially()"]
Check_LLM_Semantics_sequentially --> Ollama

%% Stage 4 internals
Stage4 --> summarize["summarize_papers()"]
summarize --> RagClass
RagClass --> |summary| summarizing_retrieved_references["summarizing_retrieved_references()"]
summarizing_retrieved_references --> Ollama
RagClass -->|writes results| Results
```
</details>

### ğŸ’  ASCII Architecture Overview
<details>
 <summary></summary>

```text
Browser (Streamlit UI)
  â†“
app.py
  â”œâ”€ UI
  â”œâ”€ writes/watches states â†â†’ ./logs/ (*.log / *.txt)
  â”œâ”€ reads results â† ./results/ (*.json / *.docx)
  â””â”€ run pipeline button
     â†“
-----------------------------------------------------
 Terminal (independent process)
  â””â”€pipeline_init.py
     â”œâ”€ watches/writes â†â†’ ./logs/ (*.log / *.txt)
     â”œâ”€ reads/writes cache â†â†’ ./cache/ (*.pkl)
     â””â”€ class_pmc_retrieval.py
         â””â”€ class PmcRetrievalPipeline    
             â”œâ”€ Stage 1 â€” Fetch papers   
             â”‚       â””â”€ process_multiple_articles()
             â”‚           â””â”€ InterpreterPoolExecutor (true multithreading)
             â”‚               â””â”€ pmc_article_parser_functions.py
             â”‚                   â”œâ”€ reads PMC articles
             â”‚                   â”œâ”€ keyword search
             â”‚                   â””â”€ text chunking   
             â”‚
             â”œâ”€ Stage 2 â€” RAG
             â”‚      â””â”€ run_in_batches()
             â”‚             â””â”€ ThreadPoolExecutor (parallel calls)
             â”‚                 â””â”€ prepare_list_to_run()
             â”‚                    â””â”€ rag_class.py
             â”‚                       â””â”€ class Refined_Semantic_matching
             â”‚                           â””â”€ run_embedding_scores()
             â”‚                               â””â”€ calls Ollama for embeddings
             â”œâ”€ Stage 3 â€” LLM Approvals 
             â”‚      â””â”€ run_LLM()
             â”‚             â””â”€ rag_class.py
             â”‚                 â””â”€ class Refined_Semantic_matching
             â”‚                    â””â”€ Check_LLM_Semantics_sequentially()
             â”‚                       â””â”€ calls Ollama for LLM approvals
             â”‚                           
             â”œâ”€ Stage 4 â€” LLM Summary 
                 â””â”€ summarize_papers()
                     â”œâ”€ rag_class.py
                     â”‚   â””â”€ class Refined_Semantic_matching
                     â”‚       â””â”€ summarizing_retrieved_references()
                     â”‚           â””â”€ calls Ollama for LLM summary (limit: 100 chunks)
                     â””â”€ writes final outputs â†’ ./results/ (*.json / *.docx)             
```
</details>

### ğŸ“ Project Structure
<details>
 <summary></summary>

```text
.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ PMC/
â”‚    â””â”€â”€ *.xml                                  # Articles as .xml files (Unicode)
â”œâ”€â”€ src/
â”‚    â”œâ”€â”€ app.py                                 # Streamlit UI
â”‚    â””â”€â”€ backend
â”‚         â”œâ”€â”€ pipeline_init.py                  # Entry point for backend pipeline
â”‚         â”œâ”€â”€ class_pmc_retrieval.py            # PmcRetrievalPipeline class
â”‚         â”œâ”€â”€ rag_class.py                      # Refined_Semantic_matching class
â”‚         â””â”€â”€ pmc_article_parser_functions.py   # Functions for parsing and scanning PMC articles
â”œâ”€â”€ cache/                                      # .pkl caches
â”‚    â””â”€â”€ *.pkl
â”œâ”€â”€ logs/                                       # Log files for debugging
     â”œâ”€â”€*.txt
     â””â”€â”€*.log                                   
â””â”€â”€ results/                                    # Final JSON / DOCX outputs
     â”œâ”€â”€*.docx
     â””â”€â”€*.json

```
</details>

---
## ğŸ“¦ Dependencies

| Component | tested version |
|-----------|-----------------|
| Ollama | 0.13.50-0.15.1
| Python | 3.14.1 |
| pip | 25.3 |
| streamlit | 1.52.2 |
| numpy | 2.3.5 |
| scikit-learn | 1.7.2 |
| python-docx | 1.2.0 |

## ğŸ“ Installation

### 1. Clone the repository 
```bash 
git clone https://github.com/daniel-sampaio-goncalves/KELLMA.git 
cd KELLMA
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```
### 3. Install and configure Ollama
Download Ollama at <https://ollama.com/download>

> **Note:**<br>
For more info on Ollama check out their [documentation](https://docs.ollama.com/)

Once Ollama installed, download the required LLM models using the following commands in your terminal:
```bash
ollama  pull  qwen3-embedding:latest
ollama  pull  gpt-oss:20b
ollama  pull  qwen3:32b
ollama  pull  gemma3:27b
ollama  pull  deepseek-r1:32b
```
Set some a optional flag before running ollama:
```ps1
# increase the request queue size for highâ€‘concurrency workloads
$env:OLLAMA_MAX_QUEUE= "4096" 
# launch ollama
ollama serve
```
### 4. Download and extract PMC articles
For licensing reasons, PMC files will not be shared on GitHub. A free, open textâ€‘mining dataset is available directly from [PubMed](https://pmc.ncbi.nlm.nih.gov/tools/textmining/) through public servers. Follow their instructions to download and extract the data. The current version was tested on curated `PMC*.xml` files containing Unicode characters. Articles can be placed in a single folder (`PMC/*.xml`) or organized across multiple subfolders (e.g., `PMC/FOLDER_1/*.xml`, `PMC/FOLDER_2/*.xml`, `PMC/FOLDER_.../*.xml`).

>**Note:**<br>
>While NTFS does not impose a strict limit on the number of files in a single directory, Windows performance can degrade significantly when a folder contains very large numbers of files. Antivirus scanning, file indexing, Explorer enumeration, and backup tools may slow down or interrupt file operations. For better reliability and performance, itâ€™s recommended to split large collections of articles into multiple folders, each containing fewer than one million files.



### 5. Run the Streamlit UI
Run the command below in a terminal using an environment that defaults to Python 3.14.1+. Once executed, your browser should open at <http://localhost:8501/>
```bash
cd src
streamlit run app.py --server.port=8501
```
>**Important:**<br>
>Using Python versions earlier than 3.14 will result in errors, because the backend relies on `InterpreterPoolExecutor` for multiprocessing which is a feature introduced in Python 3.14.

## âš¡ System Requirements
|Component | Recommended |
|-----------|-----------------|
| OS | Windows 11 64-bit|
| CPU | x64 multicore CPU|
| GPU | Nvidia RTX3090 or better |
| VRAM | > 20GB |
| RAM | > 32GB |
| Storage | NVME SSD with > 1GB/s read/writes|
| Filesystem | NTFS |



>**Notes:** <br><br>
>**Regarding multiâ€‘GPU support:**<br>
The application was built to run locally on a single workstation, but the code structure was intentionally designed to allow future support for multiple GPUs and multiple Ollama server instances for RAG and LLM approval steps. This is not implemented yet, but it may become a feature later on.<br><br>
**Regarding queue support:**<br>
The app is currently designed to handle only one request at a time to avoid overhead on smaller workstations. However, a queue system could be added in future versions for users running on faster/server hardware.<br><br>
**Regarding testing:**<br>
Currently, the application has only been tested on a single Windows 11 machine. The workload is computationally intensive, so a powerful and stable system is recommended for optimal performance.<br>
On the test system used (Intel i9-10900K CPU, Nvidia RTX 3090 GPU, 128GB DDR4 RAM, 2 TB NVMe SSD, NTFS), processing peaked at approximately 1.6GB/s, with CPU utilization near 90% on all cores during article parsing while RAM stayed below 64GB. GPU utilization was consistently staying between 90â€“100% during inference. For a given query, workload duration may vary a lot depending on how many articles are found, and depends on how many text chunks are passed through each step, with later steps being more time demanding.


## â” FAQ

| Question | Answer |
|---------|--------|
| Do I need a GPU with a lot of VRAM to run this? | No. Although this pipeline hasnâ€™t been fully tested on CPUâ€‘only setups, Ollama can run models on CPU if you have enough RAM at a significant performance cost. Any GPU supported by Ollama will in theory work and will automatically split the load between GPU and CPU when VRAM is limited. |
| I don't have any results / The pipeline ended with â€œNo articles found matching the keyword searchâ€ | This simply means no articles matched your query. Either the keyword doesnâ€™t appear in the dataset, or your XML format isnâ€™t being parsed correctly. Check the error logs for more details. |
| It's taking forever | LLM inference is slow, and GPU performance varies widely. You can reduce runtime by choosing a *rare* master keyword instead of a common one. With very large datasets, always preâ€‘filter aggressively using keyword searches as later stages (RAG/LLM inference) are far more expensive. The algorithm looks for the master keyword **plus at least one additional keyword**, so selecting a rare master keyword dramatically reduces the search space. For example, when looking for *Sox13* expression in muscle, avoid using *muscle* as the master keyword. Instead, use *sox13* as the master keyword and *muscle*,*expression* as secondary keywords, since far fewer articles mention *Sox13*.|

## ğŸ¤ Contributing
*KELLMA* is an openâ€‘source project I whipped up during some downtime to help myself and my lab colleagues screen through literature more efficiently. Itâ€™s not my fullâ€‘time job, so feel free to fork the repository, modify the app, and experiment with it.

|Next Milestones|
|-----------|
| Redesign embedding requests to Ollama for faster throughput |
| Add multiâ€‘GPU / multiâ€‘instance Ollama support |
| Improve fileâ€‘loading performance |
| Implement sequential job queueing |
| Expand parsing filters (author, date, journal, etc.) |
| Support bioRxiv text files |

## ğŸ“œ License
Apacheâ€¯2.0 â€“ see the [LICENSE](LICENSE) file for details.

---